###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.88.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "main.baml": "// main.baml\n\nclass Reply {\n  action \"reply_to_user\" @description(\"reply to the user once we have the compelte result\")\n  message string\n}\n\nclass Search {\n  action \"search_web\" @description(\"search the web for the best options\")\n  query string\n}\n\nclass Think {\n  action \"think\" @description(\"Signal the need to consult the supervisor with the given query and context before proceeding. Context is the conversation history till now\")\n  query string // The user's original query or the specific topic for supervisor review\n  context string // The conversation history and relevant data for the supervisor\n}\n\nfunction Chat(state: Message[]) -> Search | Think | Reply {\n  client \"openai/gpt-4o-mini-2024-07-18\"\n  prompt #\"\n    You are an assistant that is helping the user get the best result for their query using the Search action \n    for web search, think action to call your supervisor for review and reply action to reply to the user. The reply action should only be called if the supervisor thinks the research is complete.\n\n    {{ ctx.output_format }}\n    \n    {% for m in state %}\n    {{ _.role(m.role) }}\n    {{ m.content }}\n    {% endfor %}\n  \"#\n}\n\nfunction Thinking(query: string, context: string) -> Message {\n  client \"openai/gpt-4.1\"\n  prompt #\"\n    You are a supervisor to an assistant that is helping a user with their query: {{query}}\n    You have the following context (the conversation between the assistant, user, and potentially you):\n    {{context}}\n    Your role is to guide the assistant. Instruct them if they need to do more research (and what to look for), or if they should reply to the user with the information gathered so far.\n    Format your response as a 'Message' object with your role as 'supervisor'.\n\n    {{ ctx.output_format }}\n\n    ALWAYS listen to the Timetracker role. If it says it is time to reply to the user, ask the assistant to reply to the user.\n  \"#\n}\n\n\nclass Message {\n  role \"user\" | \"assistant\" | \"supervisor\" | \"Timetracker\"\n  content string\n}\n\n\ntest test_chat_initiates_search {\n  functions [Chat]\n  args {\n    state [\n      {\n        role \"user\"\n        content \"I am based in Vancouver, Canada but would be moving to US soon, I want to sign up for Pilates School and learn to become a Pilates instructor. I do want to just focus on Mat Pilates, I have couple of suggestions for the schools but please do your own research based on reviews, popularity which school would be the best and has credibility both in US and Canada. Suggestions: Allmethod, Core community\"\n      }\n    ]\n  }\n  @@assert(action_is_search, {{ this.action == \"search_web\" }})\n  @@assert(query_is_correct, {{ this.query == \"best Mat Pilates instructor schools US Canada reviews popularity credibility suggestions Allmethod Core community\" }})\n}\n\ntest test_chat_triggers_think_after_search {\n  functions [Chat]\n  args {\n    state [\n      {\n        role \"user\",\n        content \"I want to find the best Pilates schools.\"\n      },\n      {\n        role \"assistant\",\n        content \"The search result for 'best Pilates schools' is: Found School A (good reviews), School B (popular but expensive), School C (new, focuses on Mat Pilates).\"\n      }\n    ]\n  }\n  @@assert(action_is_think, {{ this.action == \"think\" }})\n  @@assert(think_query_is_correct, {{ this.query == \"I want to find the best Pilates schools.\" }})\n  @@assert(think_context_is_correct, {{ this.context == \"user: I want to find the best Pilates schools.\\nassistant: The search result for 'best Pilates schools' is: Found School A (good reviews), School B (popular but expensive), School C (new, focuses on Mat Pilates).\" }})\n}\n\ntest test_thinking_function {\n  functions [Thinking]\n  args {\n    query \"User query: Best Pilates schools. Search results: School A (good), School B (expensive), School C (Mat Pilates focus).\"\n    context \"User: I want to find the best Pilates schools.\\nAssistant: Search results: School A (good reviews), School B (popular but expensive), School C (new, focuses on Mat Pilates).\"\n  }\n  @@assert(role_is_supervisor, {{ this.role == \"supervisor\" }})\n  // Check if the content provides some guidance, e.g. mentions a school or next step.\n  // This assertion is kept general to avoid LLM brittleness.\n  @@assert(content_provides_guidance, {{ (\"School C\" in this.content or \"next step\" in this.content or \"research further\" in this.content or \"advise to reply\" in this.content) and this.content|length > 10 }})\n}\n\ntest test_chat_generates_reply_now_thinks_for_supervisor_approval {\n  functions [Chat]\n  args {\n    state [\n      {\n        role \"user\",\n        content \"What is the capital of France?\"\n      },\n      {\n        role \"assistant\",\n        content \"The search result for 'capital of France' is Paris. This information is confirmed by multiple sources.\"\n      }\n    ]\n  }\n  // Given the Chat prompt \"reply action if your supervisor thinks your research is complete\",\n  // and no supervisor message, the assistant should now use the Think action to consult.\n  @@assert(action_is_think, {{ this.action == \"think\" }})\n  @@assert(think_query_is_user_intent, {{ this.query == \"What is the capital of France?\" }})\n  @@assert(think_context_is_current_state, {{ \"Paris\" in this.context and \"user\" in this.context }})\n}\n\n",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return file_map