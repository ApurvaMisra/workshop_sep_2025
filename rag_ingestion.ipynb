{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System Workshop: Earnings Call Analysis\n",
        "\n",
        "Welcome to the RAG (Retrieval-Augmented Generation) workshop! In this notebook, you'll build a simple RAG system to analyze earnings call transcripts.\n",
        "\n",
        "## What you'll learn:\n",
        "1. **Data Ingestion**: Load and process earnings call transcripts\n",
        "2. **Text Chunking**: Split documents into manageable pieces\n",
        "3. **Embeddings**: Convert text to vectors\n",
        "4. **Vector Storage**: Store embeddings in Pinecone\n",
        "5. **Retrieval**: Find relevant information\n",
        "6. **Generation**: Create answers using retrieved context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Configuration\n",
        "\n",
        "First, let's install the required packages and set up our configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (7.3.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from pinecone) (2025.8.3)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from pinecone) (1.8.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from pinecone) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: packaging<25.0,>=24.2 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.5)\n",
            "Requirement already satisfied: six>=1.5 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: openai in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from openai) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: python-dotenv in /Users/apurvamisra/.pyenv/versions/3.10.10/envs/workshop/lib/python3.10/site-packages (1.1.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install pinecone \n",
        "!pip install openai\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import hashlib\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# External libraries\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#Load environment variables from .env file\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration complete!\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# TODO: Fill in your API keys and configuration\n",
        "# ============================================\n",
        "\n",
        "# Option 1: Set directly in code (for workshop)\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # TODO: Add your OpenAI API key\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")  # TODO: Add your Pinecone API key\n",
        "\n",
        "# Option 2: Use environment variables (recommended for production)\n",
        "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "#PINECONE_API_KEY= os.getenv(\"PINECONE_API_KEY\") \n",
        "\n",
        "# Configuration parameters\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"  # OpenAI embedding model\n",
        "EMBEDDING_DIMENSION = 1536  # Dimension for text-embedding-3-small\n",
        "\n",
        "# TODO: Adjust these chunking parameters\n",
        "CHUNK_SIZE = 500  # Maximum number of characters per chunk\n",
        "CHUNK_OVERLAP = 50  # Number of overlapping characters between chunks\n",
        "\n",
        "# Pinecone configuration\n",
        "PINECONE_INDEX_NAME = \"earnings-calls\"  # TODO: Choose your index name\n",
        "PINECONE_ENVIRONMENT = \"us-east-1\"  # TODO: Update based on your Pinecone region\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "print(\"‚úÖ Configuration complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Data Ingestion Pipeline\n",
        "\n",
        "### Step 1: Load Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded document: 2020-Jan-28-AAPL.txt\n",
            "Content preview: \n",
            "\n",
            "Thomson Reuters StreetEvents Event Brief\n",
            "E D I T E D   V E R S I O N\n",
            "\n",
            "Q1 2020 Apple Inc Earnings Call\n",
            "JANUARY 28, 2020 / 10:00PM GMT\n",
            "\n",
            "================================================================...\n",
            "Metadata: {'filename': '2020-Jan-28-AAPL.txt', 'filepath': 'earnings-call-transcripts/Transcripts/AAPL/2020-Jan-28-AAPL.txt', 'ticker': 'AAPL', 'date': '2020-Jan-28', 'year': '2020'}\n"
          ]
        }
      ],
      "source": [
        "def load_earnings_call(filepath: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load a single earnings call transcript.\n",
        "    \n",
        "    Args:\n",
        "        filepath: Path to the transcript file\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with 'content' and 'metadata'\n",
        "    \"\"\"\n",
        "    # Read the file content\n",
        "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    # Extract metadata from filename (format: YYYY-Mon-DD-TICKER.txt)\n",
        "    filename = os.path.basename(filepath)\n",
        "    parts = filename.replace('.txt', '').split('-')\n",
        "    \n",
        "    metadata = {\n",
        "        'filename': filename,\n",
        "        'filepath': filepath,\n",
        "        'ticker': parts[3] if len(parts) >= 4 else 'UNKNOWN',\n",
        "        'date': f\"{parts[0]}-{parts[1]}-{parts[2]}\" if len(parts) >= 3 else 'UNKNOWN',\n",
        "        'year': parts[0] if len(parts) >= 1 else 'UNKNOWN'\n",
        "    }\n",
        "    \n",
        "    return {\n",
        "        'content': content,\n",
        "        'metadata': metadata\n",
        "    }\n",
        "\n",
        "# Test loading a single document\n",
        "test_file = \"earnings-call-transcripts/Transcripts/AAPL/2020-Jan-28-AAPL.txt\"\n",
        "if os.path.exists(test_file):\n",
        "    doc = load_earnings_call(test_file)\n",
        "    print(f\"Loaded document: {doc['metadata']['filename']}\")\n",
        "    print(f\"Content preview: {doc['content'][:200]}...\")\n",
        "    print(f\"Metadata: {doc['metadata']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10 documents\n",
            "\n",
            "Sample document metadata:\n",
            "  - AAPL: 2018-May-01\n",
            "  - AAPL: 2019-Oct-30\n",
            "  - AAPL: 2016-Jan-26\n"
          ]
        }
      ],
      "source": [
        "def load_all_documents(base_path: str = \"earnings-call-transcripts/Transcripts\", \n",
        "                      tickers: List[str] = None,\n",
        "                      limit: int = None) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load multiple earnings call transcripts.\n",
        "    \n",
        "    Args:\n",
        "        base_path: Base directory containing transcripts\n",
        "        tickers: List of tickers to load (None = load all)\n",
        "        limit: Maximum number of documents to load\n",
        "    \n",
        "    Returns:\n",
        "        List of document dictionaries\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    if tickers:\n",
        "        # Load specific tickers\n",
        "        for ticker in tickers:\n",
        "            pattern = os.path.join(base_path, ticker, \"*.txt\")\n",
        "            files = glob.glob(pattern)\n",
        "            \n",
        "            for filepath in files[:limit] if limit else files:\n",
        "                doc = load_earnings_call(filepath)\n",
        "                documents.append(doc)\n",
        "    else:\n",
        "        # Load all documents\n",
        "        pattern = os.path.join(base_path, \"*\", \"*.txt\")\n",
        "        files = glob.glob(pattern)\n",
        "        \n",
        "        for filepath in files[:limit] if limit else files:\n",
        "            doc = load_earnings_call(filepath)\n",
        "            documents.append(doc)\n",
        "    \n",
        "    print(f\"Loaded {len(documents)} documents\")\n",
        "    return documents\n",
        "\n",
        "# Load sample documents for testing\n",
        "# TODO: Adjust the tickers and limit as needed\n",
        "documents = load_all_documents(tickers=['AAPL', 'MSFT'], limit=5)\n",
        "print(f\"\\nSample document metadata:\")\n",
        "for doc in documents[:3]:\n",
        "    print(f\"  - {doc['metadata']['ticker']}: {doc['metadata']['date']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Text Chunking\n",
        "\n",
        "Large documents need to be split into smaller chunks for effective retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 3 chunks from sample text\n",
            "First chunk: This is a sample text. This is a sample text. This is a sample text. This is a sample text. This is ...\n",
            "Chunk size: 500 characters\n"
          ]
        }
      ],
      "source": [
        "def simple_text_splitter(text: str, \n",
        "                        chunk_size: int = CHUNK_SIZE, \n",
        "                        chunk_overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into chunks with overlap.\n",
        "    \n",
        "    Args:\n",
        "        text: Text to split\n",
        "        chunk_size: Maximum characters per chunk\n",
        "        chunk_overlap: Overlapping characters between chunks\n",
        "    \n",
        "    Returns:\n",
        "        List of text chunks\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    \n",
        "    # TODO: Implement the chunking logic\n",
        "    # Hint: Use a sliding window approach\n",
        "    \n",
        "    # Simple implementation\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        \n",
        "        # Add chunk if it's not empty\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "        \n",
        "        # Move start position (with overlap)\n",
        "        start = end - chunk_overlap\n",
        "        \n",
        "        # Break if we've reached the end\n",
        "        if end >= len(text):\n",
        "            break\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Test the chunker\n",
        "sample_text = \"This is a sample text. \" * 50  # Create a long text\n",
        "chunks = simple_text_splitter(sample_text)\n",
        "print(f\"Created {len(chunks)} chunks from sample text\")\n",
        "print(f\"First chunk: {chunks[0][:100]}...\")\n",
        "print(f\"Chunk size: {len(chunks[0])} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 1444 chunks from 10 documents\n",
            "\n",
            "Sample chunk:\n",
            "  ID: 81e54cc8b9bfede8ca5821bb2badac3d\n",
            "  Text preview: \n",
            "\n",
            "Thomson Reuters StreetEvents Event Brief\n",
            "E D I T E D   V E R S I O N\n",
            "\n",
            "Q2 2018 Apple Inc Earnings C...\n",
            "  Metadata: {'filename': '2018-May-01-AAPL.txt', 'filepath': 'earnings-call-transcripts/Transcripts/AAPL/2018-May-01-AAPL.txt', 'ticker': 'AAPL', 'date': '2018-May-01', 'year': '2018', 'chunk_index': 0, 'total_chunks': 119}\n"
          ]
        }
      ],
      "source": [
        "def chunk_documents(documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Split all documents into chunks.\n",
        "    \n",
        "    Args:\n",
        "        documents: List of document dictionaries\n",
        "    \n",
        "    Returns:\n",
        "        List of chunk dictionaries with text and metadata\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "    \n",
        "    for doc in documents:\n",
        "        # Split document into chunks\n",
        "        text_chunks = simple_text_splitter(doc['content'])\n",
        "        \n",
        "        # Create chunk objects with metadata\n",
        "        for i, chunk_text in enumerate(text_chunks):\n",
        "            chunk_id = hashlib.md5(f\"{doc['metadata']['filename']}_{i}\".encode()).hexdigest() # Unique ID, deterministic\n",
        "            \n",
        "            chunk = {\n",
        "                'id': chunk_id,\n",
        "                'text': chunk_text,\n",
        "                'metadata': {\n",
        "                    **doc['metadata'],  # Include all document metadata\n",
        "                    'chunk_index': i,\n",
        "                    'total_chunks': len(text_chunks)\n",
        "                }\n",
        "            }\n",
        "            all_chunks.append(chunk)\n",
        "    \n",
        "    return all_chunks\n",
        "\n",
        "# Chunk all documents\n",
        "chunks = chunk_documents(documents)\n",
        "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
        "print(f\"\\nSample chunk:\")\n",
        "print(f\"  ID: {chunks[0]['id']}\")\n",
        "print(f\"  Text preview: {chunks[0]['text'][:100]}...\")\n",
        "print(f\"  Metadata: {chunks[0]['metadata']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Generate Embeddings\n",
        "\n",
        "Convert text chunks into vector embeddings using OpenAI's embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated embedding with dimension: 1536\n",
            "First 5 values: [0.020807893946766853, 0.0072983973659574986, 0.00849229283630848, -0.016416063532233238, -0.012294281274080276]\n"
          ]
        }
      ],
      "source": [
        "def get_embedding(text: str, model: str = EMBEDDING_MODEL) -> List[float]:\n",
        "    \"\"\"\n",
        "    Get embedding for a single text using OpenAI API.\n",
        "    \n",
        "    Args:\n",
        "        text: Text to embed\n",
        "        model: OpenAI embedding model to use\n",
        "    \n",
        "    Returns:\n",
        "        List of floats representing the embedding\n",
        "    \"\"\"\n",
        "    # Clean the text, more steps could be added here\n",
        "    text = text.replace(\"\\n\", \" \").strip()\n",
        "    \n",
        "    response = openai_client.embeddings.create(\n",
        "        input=text,\n",
        "        model=model\n",
        "    )\n",
        "    \n",
        "    return response.data[0].embedding\n",
        "\n",
        "# Test embedding generation\n",
        "test_text = \"This is a test sentence for embedding.\"\n",
        "test_embedding = get_embedding(test_text)\n",
        "print(f\"Generated embedding with dimension: {len(test_embedding)}\")\n",
        "print(f\"First 5 values: {test_embedding[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for 10 chunks...\n",
            "  Processed 10/10 chunks\n",
            "‚úÖ Generated embeddings for 10 chunks\n",
            "\n",
            "First embedded chunk has 1536 dimensions\n"
          ]
        }
      ],
      "source": [
        "def embed_chunks(chunks: List[Dict[str, Any]], \n",
        "                batch_size: int = 10) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Generate embeddings for all chunks.\n",
        "    \n",
        "    Args:\n",
        "        chunks: List of chunk dictionaries\n",
        "        batch_size: Number of chunks to process at once\n",
        "    \n",
        "    Returns:\n",
        "        List of chunks with embeddings added\n",
        "    \"\"\"\n",
        "    embedded_chunks = []\n",
        "    \n",
        "    print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
        "    \n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "        \n",
        "        for chunk in batch:\n",
        "            \n",
        "            try:\n",
        "                embedding = get_embedding(chunk['text'])\n",
        "                chunk['embedding'] = embedding\n",
        "                embedded_chunks.append(chunk)\n",
        "            except Exception as e:\n",
        "                print(f\"Error embedding chunk {chunk['id']}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # Progress update\n",
        "        print(f\"  Processed {min(i+batch_size, len(chunks))}/{len(chunks)} chunks\")\n",
        "        \n",
        "        # Rate limiting (to avoid hitting API limits)\n",
        "        time.sleep(0.5)\n",
        "    \n",
        "    print(f\"‚úÖ Generated embeddings for {len(embedded_chunks)} chunks\")\n",
        "    return embedded_chunks\n",
        "\n",
        "# Generate embeddings for a subset of chunks\n",
        "# TODO: Adjust the number of chunks to embed based on your API limits\n",
        "chunks_to_embed = chunks[:10]  # Start with just 10 chunks for testing\n",
        "embedded_chunks = embed_chunks(chunks_to_embed)\n",
        "print(f\"\\nFirst embedded chunk has {len(embedded_chunks[0]['embedding'])} dimensions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Store in Pinecone\n",
        "\n",
        "Initialize Pinecone and store the embedded chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating index 'earnings-calls'...\n",
            "‚úÖ Index created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/apurvamisra/.pyenv/versions/workshop/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index stats: {'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {},\n",
            " 'total_vector_count': 0,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ],
      "source": [
        "def initialize_pinecone():\n",
        "    \"\"\"\n",
        "    Initialize Pinecone client and create index if needed.\n",
        "    \n",
        "    Returns:\n",
        "        Pinecone index object\n",
        "    \"\"\"\n",
        "    # Initialize Pinecone\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "    \n",
        "    # Check if index exists\n",
        "    existing_indexes = [index.name for index in pc.list_indexes()]\n",
        "    \n",
        "    if PINECONE_INDEX_NAME not in existing_indexes:\n",
        "        # TODO: Create the index\n",
        "        # Hint: Use pc.create_index() with appropriate parameters\n",
        "        \n",
        "        print(f\"Creating index '{PINECONE_INDEX_NAME}'...\")\n",
        "        pc.create_index(\n",
        "            name=PINECONE_INDEX_NAME,\n",
        "            dimension=EMBEDDING_DIMENSION,\n",
        "            metric='cosine',  # Can also use 'euclidean' or 'dotproduct'\n",
        "            spec=ServerlessSpec(\n",
        "                cloud='aws',\n",
        "                region=PINECONE_ENVIRONMENT\n",
        "            )\n",
        "        )\n",
        "        print(f\"‚úÖ Index created\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Using existing index '{PINECONE_INDEX_NAME}'\")\n",
        "    \n",
        "    # Get the index\n",
        "    index = pc.Index(PINECONE_INDEX_NAME)\n",
        "    \n",
        "    # Wait for index to be ready\n",
        "    time.sleep(2)\n",
        "    \n",
        "    # Print index stats\n",
        "    stats = index.describe_index_stats()\n",
        "    print(f\"Index stats: {stats}\")\n",
        "    \n",
        "    return index\n",
        "\n",
        "# Initialize Pinecone\n",
        "index = initialize_pinecone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Storing 10 chunks in Pinecone...\n",
            "  Stored 10/10 chunks\n",
            "‚úÖ Successfully stored all chunks in Pinecone\n",
            "Updated index stats: {'dimension': 1536,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {},\n",
            " 'total_vector_count': 0,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ],
      "source": [
        "def store_in_pinecone(index, embedded_chunks: List[Dict[str, Any]], \n",
        "                     batch_size: int = 100):\n",
        "    \"\"\"\n",
        "    Store embedded chunks in Pinecone.\n",
        "    \n",
        "    Args:\n",
        "        index: Pinecone index object\n",
        "        embedded_chunks: List of chunks with embeddings\n",
        "        batch_size: Number of vectors to upsert at once\n",
        "    \"\"\"\n",
        "    print(f\"Storing {len(embedded_chunks)} chunks in Pinecone...\")\n",
        "    \n",
        "    for i in range(0, len(embedded_chunks), batch_size):\n",
        "        batch = embedded_chunks[i:i+batch_size]\n",
        "        \n",
        "        # Prepare vectors for upsert\n",
        "        vectors = []\n",
        "        for chunk in batch:\n",
        "            vector = {\n",
        "                'id': chunk['id'],\n",
        "                'values': chunk['embedding'],\n",
        "                'metadata': {\n",
        "                    'text': chunk['text'], \n",
        "                    'ticker': chunk['metadata']['ticker'],\n",
        "                    'date': chunk['metadata']['date'],\n",
        "                    'filename': chunk['metadata']['filename'],\n",
        "                    'chunk_index': chunk['metadata']['chunk_index']\n",
        "                }\n",
        "            }\n",
        "            vectors.append(vector)\n",
        "        \n",
        "        # Upsert to Pinecone\n",
        "        index.upsert(vectors=vectors)\n",
        "        \n",
        "        print(f\"  Stored {min(i+batch_size, len(embedded_chunks))}/{len(embedded_chunks)} chunks\")\n",
        "    \n",
        "    print(f\"‚úÖ Successfully stored all chunks in Pinecone\")\n",
        "    \n",
        "    # Print updated stats\n",
        "    time.sleep(2)  # Wait for index to update\n",
        "    stats = index.describe_index_stats()\n",
        "    print(f\"Updated index stats: {stats}\")\n",
        "\n",
        "# Store the embedded chunks\n",
        "store_in_pinecone(index, embedded_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Complete Ingestion Pipeline\n",
        "\n",
        "Now let's put it all together in a single pipeline function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingest_documents(tickers: List[str] = None, \n",
        "                    max_documents: int = None):\n",
        "    \"\"\"\n",
        "    Complete ingestion pipeline: load, chunk, embed, and store documents.\n",
        "    \n",
        "    Args:\n",
        "        tickers: List of tickers to ingest (None = all)\n",
        "        max_documents: Maximum number of documents to process\n",
        "    \"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"Starting Document Ingestion Pipeline\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Step 1: Load documents\n",
        "    print(\"\\nüìÑ Step 1: Loading documents...\")\n",
        "    documents = load_all_documents(tickers=tickers, limit=max_documents)\n",
        "    \n",
        "    if not documents:\n",
        "        print(\"No documents found!\")\n",
        "        return\n",
        "    \n",
        "    # Step 2: Chunk documents\n",
        "    print(\"\\n‚úÇÔ∏è Step 2: Chunking documents...\")\n",
        "    chunks = chunk_documents(documents)\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    \n",
        "    # Step 3: Generate embeddings\n",
        "    print(\"\\nüî¢ Step 3: Generating embeddings...\")\n",
        "    embedded_chunks = embed_chunks(chunks)\n",
        "    \n",
        "    # Step 4: Initialize Pinecone\n",
        "    print(\"\\nüîó Step 4: Initializing Pinecone...\")\n",
        "    index = initialize_pinecone()\n",
        "    \n",
        "    # Step 5: Store in Pinecone\n",
        "    print(\"\\nüíæ Step 5: Storing in Pinecone...\")\n",
        "    store_in_pinecone(index, embedded_chunks)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ Ingestion Pipeline Complete!\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    return index\n",
        "\n",
        "# TODO: Run the complete pipeline\n",
        "# Start with a small number of documents for testing\n",
        "# index = ingest_documents(tickers=['AAPL'], max_documents=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Testing the Ingestion\n",
        "\n",
        "Let's test our ingestion pipeline with a simple query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Query: What is Apple's revenue growth?\n",
            "--------------------------------------------------\n",
            "\n",
            "üìä Found 3 relevant chunks:\n",
            "\n",
            "Result 1:\n",
            "  Score: 0.6189\n",
            "  Ticker: AAPL\n",
            "  Date: 2018-May-01\n",
            "  Text preview:   2. Earnings.\n",
            "          2. Revenues:\n",
            "               1. 2Q18, $61.1b.\n",
            "                    1. Up 16% YoverY.\n",
            "                    2. Sixth consecutive qtr. of accelerating revenue growth.\n",
            "              ...\n",
            "\n",
            "Result 2:\n",
            "  Score: 0.6002\n",
            "  Ticker: AAPL\n",
            "  Date: 2018-May-01\n",
            "  Text preview:  and Japan, revenue up more than 20%.\n",
            "               4. iPhone's performance capped tremendous fiscal 1H, with $100b in iPhone revenue.\n",
            "                    1. Up $12b over last year, setting new 1H re...\n",
            "\n",
            "Result 3:\n",
            "  Score: 0.5983\n",
            "  Ticker: AAPL\n",
            "  Date: 2018-May-01\n",
            "  Text preview:           8. Had all-time record revenue from App Store, Apple Music, iCloud, Apple Pay and more.\n",
            "          3. Across all services, paid subscription surpassed 270m; up over 100m from year ago and up ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_retrieval(query: str, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Test retrieval from Pinecone.\n",
        "    \n",
        "    Args:\n",
        "        query: Query text\n",
        "        top_k: Number of results to return\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç Query: {query}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Generate embedding for query\n",
        "    query_embedding = get_embedding(query)\n",
        "    \n",
        "    # Initialize Pinecone and get index\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "    index = pc.Index(PINECONE_INDEX_NAME)\n",
        "    \n",
        "    # Query Pinecone\n",
        "    results = index.query(\n",
        "        vector=query_embedding,\n",
        "        top_k=top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"\\nüìä Found {len(results['matches'])} relevant chunks:\\n\")\n",
        "    \n",
        "    for i, match in enumerate(results['matches'], 1):\n",
        "        print(f\"Result {i}:\")\n",
        "        print(f\"  Score: {match['score']:.4f}\")\n",
        "        print(f\"  Ticker: {match['metadata']['ticker']}\")\n",
        "        print(f\"  Date: {match['metadata']['date']}\")\n",
        "        print(f\"  Text preview: {match['metadata']['text'][:200]}...\")\n",
        "        print()\n",
        "\n",
        "# Test queries\n",
        "# TODO: Uncomment and run after ingesting documents\n",
        "# test_retrieval(\"What is Apple's revenue growth?\")\n",
        "# test_retrieval(\"Tell me a#bout iPhone sales\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hybrid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Workshop Exercises\n",
        "\n",
        "Now it's your turn! Try these exercises:\n",
        "\n",
        "### Exercise 1: Improve Chunking\n",
        "- Modify the `simple_text_splitter` function to split on sentence boundaries\n",
        "- Experiment with different chunk sizes and overlaps\n",
        "\n",
        "### Exercise 2: Add More Metadata\n",
        "- Extract additional metadata from the transcripts (e.g., speaker names, Q&A sections)\n",
        "- Add this metadata to your chunks\n",
        "\n",
        "### Exercise 3: Look into hybrid search \n",
        "- Create sparse vector for the chunks (BM25/TF-IDF), https://docs.pinecone.io/guides/search/hybrid-search\n",
        "- Build both query reps the same way (dense embedding + sparse keywords), then send them together\n",
        "\n",
        "## üìö Next Steps\n",
        "\n",
        "After completing the ingestion pipeline, you can:\n",
        "1. Build the **Retrieval Pipeline** to query your data\n",
        "2. Create the **Generation Pipeline** to answer questions\n",
        "3. Combine everything into a complete RAG system"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
