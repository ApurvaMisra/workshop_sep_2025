{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System Workshop: Retrieval & Generation Pipeline\n",
        "\n",
        "Welcome to Part 2 of the RAG workshop! Now that you've ingested your data, let's build the retrieval and generation components.\n",
        "\n",
        "## What you'll learn:\n",
        "1. **Query Processing**: Prepare user queries for retrieval\n",
        "2. **Semantic Search**: Retrieve relevant chunks from Pinecone\n",
        "3. **Context Building**: Organize retrieved information\n",
        "4. **Response Generation**: Use LLMs to generate answers\n",
        "5. **RAG Orchestration**: Combine everything into a complete system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Configuration\n",
        "\n",
        "First, let's set up our environment and load the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if not already installed\n",
        "!pip install pinecone \n",
        "!pip install openai\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "import time\n",
        "\n",
        "# External libraries\n",
        "from pinecone import Pinecone\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"✅ Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Configuration complete!\n",
            "📊 Using index: earnings-calls\n",
            "🤖 Generation model: gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Configuration (same as ingestion pipeline)\n",
        "# ============================================\n",
        "\n",
        "# API Keys\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # TODO: Ensure your .env file has this\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")  # TODO: Ensure your .env file has this\n",
        "\n",
        "# Model Configuration\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"  # For query embedding\n",
        "GENERATION_MODEL = \"gpt-4o-mini\"  # TODO: Choose your generation model (gpt-4, gpt-3.5-turbo, etc.)\n",
        "\n",
        "# Retrieval Configuration\n",
        "TOP_K_RESULTS = 5  # TODO: Adjust number of chunks to retrieve\n",
        "SIMILARITY_THRESHOLD = 0.6  # TODO: Minimum similarity score for relevance\n",
        "\n",
        "# Generation Configuration\n",
        "MAX_CONTEXT_LENGTH = 3000  # TODO: Maximum characters of context to include\n",
        "TEMPERATURE = 0.0  # TODO: Adjust creativity (0=deterministic, 1=creative)\n",
        "MAX_TOKENS = 500  # TODO: Maximum length of generated response\n",
        "\n",
        "# Pinecone Configuration\n",
        "PINECONE_INDEX_NAME = \"earnings-calls\"  # Should match your ingestion pipeline\n",
        "\n",
        "# Initialize clients\n",
        "openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(PINECONE_INDEX_NAME)\n",
        "\n",
        "print(\"✅ Configuration complete!\")\n",
        "print(f\"📊 Using index: {PINECONE_INDEX_NAME}\")\n",
        "print(f\"🤖 Generation model: {GENERATION_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Query Processing\n",
        "\n",
        "### Step 1: Query Enhancement\n",
        "Before searching, we can enhance queries to improve retrieval quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: iPhone sales\n",
            "Enhanced: iPhone sales earnings financial results\n"
          ]
        }
      ],
      "source": [
        "def enhance_query(query: str, add_context: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Enhance a user query for better retrieval.\n",
        "    \n",
        "    Args:\n",
        "        query: Original user query\n",
        "        add_context: Whether to add contextual keywords\n",
        "    \n",
        "    Returns:\n",
        "        Enhanced query string\n",
        "    \"\"\"\n",
        "    # TODO: Implement query enhancement strategies\n",
        "    # Ideas:\n",
        "    # - Add domain-specific keywords\n",
        "    # - Expand acronyms\n",
        "    # - Add synonyms\n",
        "    # - Use a small LLM to rephrase the query\n",
        "    \n",
        "    enhanced = query\n",
        "    \n",
        "    if add_context:\n",
        "        # Add earnings call context if not present\n",
        "        keywords = ['earnings', 'revenue', 'financial', 'quarter', 'fiscal']\n",
        "        if not any(keyword in query.lower() for keyword in keywords):\n",
        "            enhanced = f\"{query} earnings financial results\"\n",
        "    \n",
        "    return enhanced\n",
        "\n",
        "# Test query enhancement\n",
        "test_query = \"iPhone sales\"\n",
        "enhanced = enhance_query(test_query)\n",
        "print(f\"Original: {test_query}\")\n",
        "print(f\"Enhanced: {enhanced}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Generated embedding with 1536 dimensions\n"
          ]
        }
      ],
      "source": [
        "def generate_query_embedding(query: str) -> List[float]:\n",
        "    \"\"\"\n",
        "    Generate embedding for a query using OpenAI.\n",
        "    \n",
        "    Args:\n",
        "        query: Query text\n",
        "    \n",
        "    Returns:\n",
        "        Query embedding vector\n",
        "    \"\"\"\n",
        "    # Clean the query\n",
        "    query = query.replace(\"\\n\", \" \").strip()\n",
        "    \n",
        "    # Generate embedding\n",
        "    response = openai_client.embeddings.create(\n",
        "        input=query,\n",
        "        model=EMBEDDING_MODEL\n",
        "    )\n",
        "    \n",
        "    return response.data[0].embedding\n",
        "\n",
        "# Test embedding generation\n",
        "test_embedding = generate_query_embedding(\"What is Apple's revenue?\")\n",
        "print(f\"✅ Generated embedding with {len(test_embedding)} dimensions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Retrieval Pipeline\n",
        "\n",
        "### Step 2: Semantic Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class RetrievedChunk:\n",
        "    \"\"\"Data class for retrieved chunks\"\"\"\n",
        "    text: str\n",
        "    score: float\n",
        "    metadata: Dict[str, Any]\n",
        "    \n",
        "    def __str__(self):\n",
        "        return f\"[Score: {self.score:.3f}] {self.metadata.get('ticker', 'N/A')} - {self.metadata.get('date', 'N/A')}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Query: What is Apple's revenue growth?\n",
            "📊 Retrieved 3 chunks:\n",
            "\n",
            "1. [Score: 0.619] AAPL - 2018-May-01\n",
            "   Preview:   2. Earnings.\n",
            "          2. Revenues:\n",
            "               1. 2Q18, $61.1b.\n",
            "                    1. Up 16% ...\n",
            "\n",
            "2. [Score: 0.600] AAPL - 2018-May-01\n",
            "   Preview:  and Japan, revenue up more than 20%.\n",
            "               4. iPhone's performance capped tremendous fisca...\n",
            "\n",
            "3. [Score: 0.598] AAPL - 2018-May-01\n",
            "   Preview:           8. Had all-time record revenue from App Store, Apple Music, iCloud, Apple Pay and more.\n",
            "  ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def retrieve_relevant_chunks(query: str, \n",
        "                           top_k: int = TOP_K_RESULTS,\n",
        "                           filter_dict: Dict = None) -> List[RetrievedChunk]:\n",
        "    \"\"\"\n",
        "    Retrieve relevant chunks from Pinecone.\n",
        "    \n",
        "    Args:\n",
        "        query: User query\n",
        "        top_k: Number of results to retrieve\n",
        "        filter_dict: Optional metadata filters (e.g., {'ticker': 'AAPL'})\n",
        "    \n",
        "    Returns:\n",
        "        List of retrieved chunks with scores and metadata\n",
        "    \"\"\"\n",
        "    # Generate query embedding\n",
        "    query_embedding = generate_query_embedding(query)\n",
        "    \n",
        "    # Query Pinecone\n",
        "    # TODO: Add metadata filtering if needed\n",
        "    results = index.query(\n",
        "        vector=query_embedding,\n",
        "        top_k=top_k,\n",
        "        include_metadata=True,\n",
        "        filter=filter_dict  # Optional filtering\n",
        "    )\n",
        "    \n",
        "    # Process results into RetrievedChunk objects\n",
        "    chunks = []\n",
        "    for match in results['matches']:\n",
        "        chunk = RetrievedChunk(\n",
        "            text=match['metadata'].get('text', ''),\n",
        "            score=match['score'],\n",
        "            metadata=match['metadata']\n",
        "        )\n",
        "        chunks.append(chunk)\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Test retrieval\n",
        "test_query = \"What is Apple's revenue growth?\"\n",
        "chunks = retrieve_relevant_chunks(test_query, top_k=3)\n",
        "\n",
        "print(f\"\\n🔍 Query: {test_query}\")\n",
        "print(f\"📊 Retrieved {len(chunks)} chunks:\\n\")\n",
        "for i, chunk in enumerate(chunks, 1):\n",
        "    print(f\"{i}. {chunk}\")\n",
        "    print(f\"   Preview: {chunk.text[:100]}...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Filtered 3 chunks to 2 relevant chunks\n",
            "   (Using threshold: 0.6)\n",
            "\n",
            "Relevant chunks: 2\n"
          ]
        }
      ],
      "source": [
        "def filter_chunks_by_relevance(chunks: List[RetrievedChunk], \n",
        "                              threshold: float = SIMILARITY_THRESHOLD) -> List[RetrievedChunk]:\n",
        "    \"\"\"\n",
        "    Filter chunks based on similarity score threshold.\n",
        "    \n",
        "    Args:\n",
        "        chunks: List of retrieved chunks\n",
        "        threshold: Minimum similarity score\n",
        "    \n",
        "    Returns:\n",
        "        Filtered list of relevant chunks\n",
        "    \"\"\"\n",
        "    # TODO: Implement relevance filtering\n",
        "    # Consider:\n",
        "    # - Similarity score threshold\n",
        "    # - Date recency\n",
        "    # - Source diversity, might not be relevant in this particular case :)\n",
        "    \n",
        "    relevant_chunks = [chunk for chunk in chunks if chunk.score >= threshold]\n",
        "    \n",
        "    print(f\"✅ Filtered {len(chunks)} chunks to {len(relevant_chunks)} relevant chunks\")\n",
        "    print(f\"   (Using threshold: {threshold})\")\n",
        "    \n",
        "    return relevant_chunks\n",
        "\n",
        "# Test filtering\n",
        "filtered_chunks = filter_chunks_by_relevance(chunks, threshold=0.6)\n",
        "print(f\"\\nRelevant chunks: {len(filtered_chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Context Building\n",
        "\n",
        "### Step 3: Organize Retrieved Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 Built context with 2 chunks\n",
            "   Total length: 1049 characters\n",
            "\n",
            "Context preview:\n",
            "[AAPL - 2018-May-01]\n",
            "  2. Earnings.\n",
            "          2. Revenues:\n",
            "               1. 2Q18, $61.1b.\n",
            "                    1. Up 16% YoverY.\n",
            "                    2. Sixth consecutive qtr. of accelerating revenue growth.\n",
            "               2. Broad-based performance with:\n",
            "                    1. iPhone up 14%.\n",
            "                    2. Services up 31%.\n",
            "                    3. Wearables up almost 50%.\n",
            "               3. Grew in each geographic segment.\n",
            "                    1. In Greater China and Japan, revenue up more t...\n"
          ]
        }
      ],
      "source": [
        "def build_context(chunks: List[RetrievedChunk], \n",
        "                 max_length: int = MAX_CONTEXT_LENGTH) -> str:\n",
        "    \"\"\"\n",
        "    Build context string from retrieved chunks.\n",
        "    \n",
        "    Args:\n",
        "        chunks: List of retrieved chunks\n",
        "        max_length: Maximum context length in characters\n",
        "    \n",
        "    Returns:\n",
        "        Formatted context string\n",
        "    \"\"\"\n",
        "    # TODO: Implement smart context building\n",
        "    # Consider:\n",
        "    # - Chunk ordering (by score, date, etc.)\n",
        "    # - Deduplication\n",
        "    # - Length constraints\n",
        "    \n",
        "    context_parts = []\n",
        "    current_length = 0\n",
        "    \n",
        "    for chunk in chunks:\n",
        "        # Format each chunk with metadata\n",
        "        chunk_text = f\"[{chunk.metadata.get('ticker', 'N/A')} - {chunk.metadata.get('date', 'N/A')}]\\n{chunk.text}\\n\"\n",
        "        \n",
        "        # Check if adding this chunk exceeds max length\n",
        "        if current_length + len(chunk_text) > max_length:\n",
        "            # Truncate if necessary\n",
        "            remaining = max_length - current_length\n",
        "            if remaining > 100:  # Only add if we have reasonable space\n",
        "                chunk_text = chunk_text[:remaining] + \"...\"\n",
        "                context_parts.append(chunk_text)\n",
        "            break\n",
        "        \n",
        "        context_parts.append(chunk_text)\n",
        "        current_length += len(chunk_text)\n",
        "    \n",
        "    context = \"\\n---\\n\".join(context_parts)\n",
        "    \n",
        "    print(f\"📝 Built context with {len(context_parts)} chunks\")\n",
        "    print(f\"   Total length: {len(context)} characters\")\n",
        "    \n",
        "    return context\n",
        "\n",
        "# Test context building\n",
        "if filtered_chunks:\n",
        "    context = build_context(filtered_chunks)\n",
        "    print(f\"\\nContext preview:\\n{context[:500]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'chunk_index': 6.0, 'date': '2018-May-01', 'filename': '2018-May-01-AAPL.txt', 'text': '  2. Earnings.\\n          2. Revenues:\\n               1. 2Q18, $61.1b.\\n                    1. Up 16% YoverY.\\n                    2. Sixth consecutive qtr. of accelerating revenue growth.\\n               2. Broad-based performance with:\\n                    1. iPhone up 14%.\\n                    2. Services up 31%.\\n                    3. Wearables up almost 50%.\\n               3. Grew in each geographic segment.\\n                    1. In Greater China and Japan, revenue up more than 20%.\\n            ', 'ticker': 'AAPL'}\n",
            "{'chunk_index': 7.0, 'date': '2018-May-01', 'filename': '2018-May-01-AAPL.txt', 'text': \" and Japan, revenue up more than 20%.\\n               4. iPhone's performance capped tremendous fiscal 1H, with $100b in iPhone revenue.\\n                    1. Up $12b over last year, setting new 1H record.\\n                    2. Highest 1H growth rate in three years.\\n               5. iPhone gained share based on IDC's latest estimates for global smartphone market.\\n                    1. Customers chose iPhone X more than any other iPhone each week in March qtr., just as they did following its l\", 'ticker': 'AAPL'}\n",
            "📚 Sources:\n",
            "- AAPL Earnings Call (2018-May-01)\n"
          ]
        }
      ],
      "source": [
        "def format_source_citations(chunks: List[RetrievedChunk]) -> str:\n",
        "    \"\"\"\n",
        "    Format source citations from retrieved chunks.\n",
        "    \n",
        "    Args:\n",
        "        chunks: List of retrieved chunks\n",
        "    \n",
        "    Returns:\n",
        "        Formatted citations string\n",
        "    \"\"\"\n",
        "    citations = []\n",
        "    seen = set()\n",
        "    \n",
        "    for chunk in chunks:\n",
        "        # Create unique citation key\n",
        "        print(chunk.metadata)\n",
        "        ticker = chunk.metadata.get('ticker', 'Unknown')\n",
        "        date = chunk.metadata.get('date', 'Unknown')\n",
        "        citation_key = f\"{ticker}_{date}\"\n",
        "        \n",
        "        # Avoid duplicates\n",
        "        #TODO: \n",
        "        #1. Can go more granular if needed (e.g., section)\n",
        "        #2. Citation can also be a webpage url if available and saved as metadata\n",
        "        if citation_key not in seen:\n",
        "            seen.add(citation_key)\n",
        "            citations.append(f\"- {ticker} Earnings Call ({date})\")\n",
        "    \n",
        "    return \"\\n\".join(citations)\n",
        "\n",
        "# Test citation formatting\n",
        "if filtered_chunks:\n",
        "    citations = format_source_citations(filtered_chunks)\n",
        "    print(\"📚 Sources:\")\n",
        "    print(citations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Generation Pipeline\n",
        "\n",
        "### Step 4: Response Generation with LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 Sample prompt:\n",
            "You are a financial analyst assistant. Answer the question based on the provided earnings call transcript excerpts.\n",
            "\n",
            "Context from earnings calls:\n",
            "[AAPL - 2018-May-01]\n",
            "  2. Earnings.\n",
            "          2. Revenues:\n",
            "               1. 2Q18, $61.1b.\n",
            "                    1. Up 16% YoverY.\n",
            "                    2. Sixth consecutive qtr. of accelerating revenue growth.\n",
            "               2. Broad-based performance with:...\n"
          ]
        }
      ],
      "source": [
        "def create_prompt(query: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Create a prompt for the LLM with query and context.\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "        context: Retrieved context\n",
        "    \n",
        "    Returns:\n",
        "        Formatted prompt string\n",
        "    \"\"\"\n",
        "    # TODO: Customize this prompt template for your use case\n",
        "    prompt = f\"\"\"You are a financial analyst assistant. Answer the question based on the provided earnings call transcript excerpts.\n",
        "\n",
        "Context from earnings calls:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Instructions:\n",
        "- Base your answer only on the provided context\n",
        "- If the context doesn't contain enough information, say so\n",
        "- Be specific and cite the company and date when referencing information\n",
        "- Keep the answer concise but comprehensive\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "# Test prompt creation\n",
        "if filtered_chunks:\n",
        "    test_prompt = create_prompt(\"What is the revenue growth for Apple in the year 2018?\", context[:500])\n",
        "    print(\"📝 Sample prompt:\")\n",
        "    print(test_prompt[:400] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Generated Response:\n",
            "The revenue growth for Apple Inc. (AAPL) in the second quarter of 2018 (2Q18) was 16% year-over-year, with total revenues reaching $61.1 billion. This information is from the earnings call on May 1, 2018.\n"
          ]
        }
      ],
      "source": [
        "def generate_response(query: str, \n",
        "                     context: str,\n",
        "                     model: str = GENERATION_MODEL,\n",
        "                     temperature: float = TEMPERATURE,\n",
        "                     max_tokens: int = MAX_TOKENS) -> str:\n",
        "    \"\"\"\n",
        "    Generate a response using OpenAI's GPT model.\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "        context: Retrieved context\n",
        "        model: OpenAI model to use\n",
        "        temperature: Creativity parameter\n",
        "        max_tokens: Maximum response length\n",
        "    \n",
        "    Returns:\n",
        "        Generated response\n",
        "    \"\"\"\n",
        "    # Create the prompt\n",
        "    prompt = create_prompt(query, context)\n",
        "    \n",
        "    # TODO: Add error handling for API calls\n",
        "    try:\n",
        "        # Call OpenAI API\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful financial analyst assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "        \n",
        "        # Extract the response\n",
        "        answer = response.choices[0].message.content\n",
        "        \n",
        "        return answer\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error generating response: {e}\")\n",
        "        return \"I encountered an error while generating the response. Please try again.\"\n",
        "\n",
        "# Test response generation\n",
        "if filtered_chunks:\n",
        "    test_response = generate_response(\n",
        "        \"What is the revenue growth?\",\n",
        "        context,\n",
        "        temperature=0.5  # Lower temperature for more focused answers\n",
        "    )\n",
        "    print(\"🤖 Generated Response:\")\n",
        "    print(test_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Complete RAG Pipeline\n",
        "\n",
        "### Step 5: Orchestrate Everything Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class RAGResponse:\n",
        "    \"\"\"Complete RAG response with all components\"\"\"\n",
        "    query: str\n",
        "    answer: str\n",
        "    sources: str\n",
        "    chunks_retrieved: int\n",
        "    chunks_used: int\n",
        "    \n",
        "    def __str__(self):\n",
        "        return f\"\"\"\\n{'='*50}\n",
        "📝 Query: {self.query}\n",
        "{'='*50}\n",
        "\n",
        "💡 Answer:\n",
        "{self.answer}\n",
        "\n",
        "📚 Sources:\n",
        "{self.sources}\n",
        "\n",
        "📊 Statistics:\n",
        "- Chunks retrieved: {self.chunks_retrieved}\n",
        "- Chunks used: {self.chunks_used}\n",
        "{'='*50}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 Starting RAG Pipeline\n",
            "   Query: What is Apple's revenue growth in 2018?\n",
            "   Enhanced: What is Apple's revenue growth in 2018?\n",
            "\n",
            "📥 Retrieving chunks...\n",
            "\n",
            "🔍 Filtering chunks...\n",
            "✅ Filtered 5 chunks to 2 relevant chunks\n",
            "   (Using threshold: 0.6)\n",
            "\n",
            "📝 Building context...\n",
            "📝 Built context with 2 chunks\n",
            "   Total length: 1049 characters\n",
            "\n",
            "🤖 Generating response...\n",
            "{'chunk_index': 6.0, 'date': '2018-May-01', 'filename': '2018-May-01-AAPL.txt', 'text': '  2. Earnings.\\n          2. Revenues:\\n               1. 2Q18, $61.1b.\\n                    1. Up 16% YoverY.\\n                    2. Sixth consecutive qtr. of accelerating revenue growth.\\n               2. Broad-based performance with:\\n                    1. iPhone up 14%.\\n                    2. Services up 31%.\\n                    3. Wearables up almost 50%.\\n               3. Grew in each geographic segment.\\n                    1. In Greater China and Japan, revenue up more than 20%.\\n            ', 'ticker': 'AAPL'}\n",
            "{'chunk_index': 7.0, 'date': '2018-May-01', 'filename': '2018-May-01-AAPL.txt', 'text': \" and Japan, revenue up more than 20%.\\n               4. iPhone's performance capped tremendous fiscal 1H, with $100b in iPhone revenue.\\n                    1. Up $12b over last year, setting new 1H record.\\n                    2. Highest 1H growth rate in three years.\\n               5. iPhone gained share based on IDC's latest estimates for global smartphone market.\\n                    1. Customers chose iPhone X more than any other iPhone each week in March qtr., just as they did following its l\", 'ticker': 'AAPL'}\n",
            "\n",
            "==================================================\n",
            "📝 Query: What is Apple's revenue growth in 2018?\n",
            "==================================================\n",
            "\n",
            "💡 Answer:\n",
            "Apple's revenue growth in 2018, specifically for the second quarter (2Q18), was 16% year-over-year, with total revenues reaching $61.1 billion. This information is sourced from the earnings call on May 1, 2018. Additionally, the company experienced broad-based performance across its product lines, with notable growth in services (31%) and wearables (almost 50%).\n",
            "\n",
            "📚 Sources:\n",
            "- AAPL Earnings Call (2018-May-01)\n",
            "\n",
            "📊 Statistics:\n",
            "- Chunks retrieved: 5\n",
            "- Chunks used: 2\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "def rag_pipeline(query: str,\n",
        "                top_k: int = TOP_K_RESULTS,\n",
        "                threshold: float = SIMILARITY_THRESHOLD,\n",
        "                filter_dict: Dict = None,\n",
        "                enhance: bool = True) -> RAGResponse:\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: retrieve, build context, and generate response.\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "        top_k: Number of chunks to retrieve\n",
        "        threshold: Similarity threshold for filtering\n",
        "        filter_dict: Optional metadata filters\n",
        "        enhance: Whether to enhance the query\n",
        "    \n",
        "    Returns:\n",
        "        RAGResponse object with answer and metadata\n",
        "    \"\"\"\n",
        "    print(f\"\\n🚀 Starting RAG Pipeline\")\n",
        "    print(f\"   Query: {query}\")\n",
        "    \n",
        "    # Step 1: Enhance query (optional)\n",
        "    if enhance:\n",
        "        query_enhanced = enhance_query(query)\n",
        "        print(f\"   Enhanced: {query_enhanced}\")\n",
        "    else:\n",
        "        query_enhanced = query\n",
        "    \n",
        "    # Step 2: Retrieve relevant chunks\n",
        "    print(f\"\\n📥 Retrieving chunks...\")\n",
        "    chunks = retrieve_relevant_chunks(query_enhanced, top_k=top_k, filter_dict=filter_dict)\n",
        "    chunks_retrieved = len(chunks)\n",
        "    \n",
        "    # Step 3: Filter by relevance\n",
        "    print(f\"\\n🔍 Filtering chunks...\")\n",
        "    relevant_chunks = filter_chunks_by_relevance(chunks, threshold=threshold)\n",
        "    chunks_used = len(relevant_chunks)\n",
        "    \n",
        "    if not relevant_chunks:\n",
        "        return RAGResponse(\n",
        "            query=query,\n",
        "            answer=\"I couldn't find relevant information to answer your question.\",\n",
        "            sources=\"No relevant sources found.\",\n",
        "            chunks_retrieved=chunks_retrieved,\n",
        "            chunks_used=0\n",
        "        )\n",
        "    \n",
        "    # Step 4: Build context\n",
        "    print(f\"\\n📝 Building context...\")\n",
        "    context = build_context(relevant_chunks)\n",
        "    \n",
        "    # Step 5: Generate response\n",
        "    print(f\"\\n🤖 Generating response...\")\n",
        "    answer = generate_response(query, context)\n",
        "    \n",
        "    # Step 6: Format citations\n",
        "    sources = format_source_citations(relevant_chunks)\n",
        "    \n",
        "    # Create response object\n",
        "    response = RAGResponse(\n",
        "        query=query,\n",
        "        answer=answer,\n",
        "        sources=sources,\n",
        "        chunks_retrieved=chunks_retrieved,\n",
        "        chunks_used=chunks_used\n",
        "    )\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Test the complete pipeline\n",
        "test_query = \"What is Apple's revenue growth in 2018?\"\n",
        "response = rag_pipeline(test_query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 Next Steps\n",
        "\n",
        "After completing the retrieval and generation pipeline:\n",
        "1. **Optimize Performance**: Cache embeddings, batch processing\n",
        "2. **Add Advanced Features**: Multi-turn conversations, feedback loops\n",
        "3. **Deploy**: Create an API or web interface\n",
        "4. **Monitor**: Track usage, performance, and accuracy\n",
        "\n",
        "## 🔑 Key Takeaways\n",
        "\n",
        "- **Query Processing**: Enhancement improves retrieval quality\n",
        "- **Semantic Search**: Vector similarity finds relevant content\n",
        "- **Context Building**: Smart organization improves generation\n",
        "- **Prompt Engineering**: Good prompts lead to better answers\n",
        "- **Pipeline Design**: Modular components allow easy improvements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
